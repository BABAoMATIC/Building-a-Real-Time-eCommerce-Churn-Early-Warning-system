# Random Event Producer

This directory contains a Kafka producer that sends random user events to the `user-events` topic every 3 seconds.

## Files

- **`random_event_producer.py`** - Main producer script
- **`test_random_producer.py`** - Test script to send sample events
- **`test_random_consumer.py`** - Consumer script to verify events
- **`RANDOM_PRODUCER_README.md`** - This documentation

## Event Format

Each event sent to the `user-events` topic contains:

```json
{
    "user_id": 123,
    "event_type": "add_to_cart",
    "timestamp": "2024-01-15T10:30:00.123456",
    "metadata": {
        "product_id": "PROD-0042",
        "quantity": 2,
        "price": 45.99,
        "session_length": 12.5,
        "page": "/products"
    }
}
```

### Event Types

The producer randomly selects from these event types:
- `add_to_cart`
- `bounce`
- `product_view`

### User IDs

- Random integers from 1 to 1000
- Used as Kafka message keys for partitioning

### Metadata Fields

#### Add to Cart Events
```json
{
    "product_id": "PROD-0042",
    "quantity": 2,
    "price": 45.99,
    "session_length": 12.5,
    "page": "/products"
}
```

#### Bounce Events
```json
{
    "page": "/checkout",
    "session_length": 1.2,
    "referrer": "google",
    "device": "mobile"
}
```

#### Product View Events
```json
{
    "product_id": "PROD-0087",
    "category": "electronics",
    "price": 299.99,
    "view_duration": 45.5,
    "page": "/product-detail",
    "device": "desktop"
}
```

## Usage

### Prerequisites

1. **Kafka Server**: Make sure Kafka is running on `localhost:9092`
2. **Topic**: Create the `user-events` topic (or it will be auto-created)
3. **Dependencies**: Install required packages

```bash
pip install kafka-python
```

### Running the Producer

```bash
python random_event_producer.py
```

This will:
- Connect to Kafka on `localhost:9092`
- Send events to the `user-events` topic
- Send one event every 3 seconds
- Log each event to the console
- Continue until stopped with Ctrl+C

### Testing the Producer

Send a few test events:

```bash
python test_random_producer.py
```

### Consuming Events

To verify events are being sent, run the consumer:

```bash
python test_random_consumer.py
```

## Configuration

You can modify the configuration in the `main()` function:

```python
KAFKA_SERVERS = 'localhost:9092'  # Kafka broker address
TOPIC = 'user-events'             # Topic name
INTERVAL_SECONDS = 3              # Seconds between events
```

## Console Output

The producer logs each event to the console:

```
2024-01-15 10:30:00,123 - INFO - Event sent - User: 123, Type: add_to_cart, Metadata: {"product_id":"PROD-0042","quantity":2,"price":45.99}, Partition: 0
2024-01-15 10:30:03,456 - INFO - Total events sent: 1
```

## Docker Integration

### Using Docker Compose

Start the random event producer with Docker:

```bash
# Start with random event producer
docker-compose --profile random up -d

# Or start everything including random producer
docker-compose --profile random --profile frontend up -d
```

### Docker Service Configuration

The producer is configured as a Docker service:

```yaml
random-event-producer:
  build:
    context: ./kafka
    dockerfile: Dockerfile
  container_name: churn-random-producer
  restart: unless-stopped
  depends_on:
    kafka:
      condition: service_healthy
  environment:
    KAFKA_BROKER: kafka:29092
    KAFKA_TOPIC: user-events
  command: ["python", "random_event_producer.py"]
  profiles:
    - random
```

## Integration with Churn Prediction System

The random events generated by this producer can be consumed by:

1. **Kafka Consumer** - Processes events and calls Flask API
2. **Flask API** - Generates churn predictions
3. **Database** - Stores prediction results
4. **Frontend** - Displays real-time analytics

### Event Flow

```
Random Producer → Kafka Topic → Consumer → Flask API → Database
```

## Error Handling

The producer includes robust error handling:
- **Connection errors**: Retries with backoff
- **Send failures**: Logs errors and continues
- **Kafka errors**: Graceful error handling
- **Keyboard interrupt**: Clean shutdown

## Monitoring

### Log Monitoring

Monitor the producer logs for:
- Event generation rate
- Error rates
- Connection status
- Message delivery confirmations

### Kafka Monitoring

Use Kafka tools to monitor the topic:

```bash
# List topics
kafka-topics --bootstrap-server localhost:9092 --list

# Describe topic
kafka-topics --describe --topic user-events --bootstrap-server localhost:9092

# Consume messages
kafka-console-consumer --topic user-events --from-beginning --bootstrap-server localhost:9092
```

## Customization

### Adding New Event Types

1. Add new event type to `self.event_types` list
2. Add metadata generation logic in `_generate_metadata()` method
3. Update documentation

### Modifying Event Frequency

Change the `INTERVAL_SECONDS` value:

```python
# Send events every 1 second
INTERVAL_SECONDS = 1

# Send events every 10 seconds
INTERVAL_SECONDS = 10
```

### Customizing Metadata

Modify the metadata generation methods:

```python
def _generate_metadata(self, event_type):
    if event_type == "custom_event":
        return {
            "custom_field": "custom_value",
            "another_field": random.randint(1, 100)
        }
    # ... existing logic
```

## Performance Considerations

### High-Volume Scenarios

For high-volume event generation:

1. **Batch Processing**: Send multiple events in batches
2. **Async Processing**: Use async/await for better performance
3. **Connection Pooling**: Reuse Kafka connections
4. **Compression**: Enable message compression

### Resource Usage

- **Memory**: Minimal memory usage
- **CPU**: Low CPU usage for event generation
- **Network**: Depends on event frequency and size

## Troubleshooting

### Common Issues

1. **Connection refused**: Make sure Kafka is running on `localhost:9092`
2. **Topic not found**: The topic will be auto-created, or create it manually
3. **Import errors**: Install kafka-python: `pip install kafka-python`

### Debug Mode

Enable debug logging:

```python
logging.basicConfig(level=logging.DEBUG)
```

### Kafka Setup

If you need to set up Kafka locally:

```bash
# Download and start Kafka
wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz
tar -xzf kafka_2.13-2.8.0.tgz
cd kafka_2.13-2.8.0

# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka (in another terminal)
bin/kafka-server-start.sh config/server.properties

# Create topic (optional - will be auto-created)
bin/kafka-topics.sh --create --topic user-events --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

## Security Considerations

- Use environment variables for sensitive configuration
- Implement authentication if needed
- Monitor for unusual activity patterns
- Use encrypted connections in production

## Production Deployment

For production deployment:

1. **Use environment variables** for configuration
2. **Enable SSL/TLS** for secure connections
3. **Set up monitoring** and alerting
4. **Use proper logging** levels
5. **Implement health checks**
6. **Use container orchestration** (Kubernetes, Docker Swarm)
